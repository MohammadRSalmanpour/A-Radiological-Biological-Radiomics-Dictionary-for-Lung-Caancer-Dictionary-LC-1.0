{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"mount_file_id":"16E_Akw4VS-89eFHC1ExemImblE7taiXC","authorship_tag":"ABX9TyOBvS1ZnTHaMn9YgHcSxuu+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","\n","TRAIN_FILE = \"/content/drive/MyDrive/Projects/Ongoing Projects/Lung Outcome Prediciton_P20250048/Data/Binarize_With_Median/OS/TH-2Y/Test_LungCT_Diagnosis_And_NSCLC_Radiogenomics/train.xlsx\"\n","TEST_FILE = \"/content/drive/MyDrive/Projects/Ongoing Projects/Lung Outcome Prediciton_P20250048/Data/Binarize_With_Median/OS/TH-2Y/Test_LungCT_Diagnosis_And_NSCLC_Radiogenomics/test.xlsx\"\n","# =========================\n","# Load data\n","# =========================\n","train = pd.read_excel(TRAIN_FILE)\n","test = pd.read_excel(TEST_FILE)"],"metadata":{"id":"4yOLtqV3LDL6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Pseudo-labeling with GridSearch, Final Models = LogisticRegression + DecisionTree\n","import numpy as np\n","\n","from sklearn.model_selection import GridSearchCV, cross_val_score\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.impute import SimpleImputer\n","from sklearn.pipeline import Pipeline\n","from sklearn.compose import ColumnTransformer\n","\n","from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, chi2, f_regression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n","from xgboost import XGBClassifier\n","from lightgbm import LGBMClassifier\n","from sklearn.feature_selection import (\n","    SelectKBest, f_classif, chi2, mutual_info_classif,\n","    RFE, SelectFromModel, VarianceThreshold, SelectPercentile,\n","    GenericUnivariateSelect, SelectFpr, SelectFdr, SelectFwe\n",")\n","from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n","from sklearn.ensemble import (\n","    RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier,\n","    AdaBoostClassifier, BaggingClassifier\n",")\n","import warnings\n","\n","import shap\n","import os\n","warnings.filterwarnings(\"ignore\")\n","\n","id_col = \"PatientID\"\n","outcome_col = \"Outcome\"\n","feature_cols = [c for c in train.columns if c not in [id_col, outcome_col]]\n","\n","# Split train\n","train_labeled = train[train[outcome_col].notnull()].copy()\n","train_unlabeled = train[train[outcome_col].isnull()].copy()\n","\n","X_labeled = train_labeled[feature_cols]\n","y_labeled = train_labeled[outcome_col].astype(int)\n","X_unlabeled = train_unlabeled[feature_cols]\n","\n","X_test = test[feature_cols]\n","y_test = test[outcome_col].astype(int)\n","\n","# =========================\n","# Preprocessing\n","# =========================\n","num_transformer = Pipeline(steps=[\n","    ('imputer', SimpleImputer(strategy='mean')),\n","    ('scaler', StandardScaler())\n","])\n","preprocessor = ColumnTransformer(\n","    transformers=[('num', num_transformer, feature_cols)]\n",")\n","k=10\n","# =========================\n","# Feature selection\n","# =========================\n","feature_selectors = {\n","\n","        'ANOVA_F': SelectKBest(score_func=f_classif, k=k),\n","        # 'Mutual_Info': SelectKBest(score_func=mutual_info_classif, k=k),\n","        'RFE_LogReg': RFE(LogisticRegression(random_state=42, max_iter=1000), n_features_to_select=k),\n","        'Random_Forest_Importance': SelectFromModel(RandomForestClassifier(random_state=42, n_estimators=100), max_features=k),\n","        # 'Extra_Trees_Importance': SelectFromModel(ExtraTreesClassifier(random_state=42, n_estimators=100), max_features=k),\n","        # 'L1_Regularization': SelectFromModel(LogisticRegression(penalty='l1', solver='liblinear', random_state=42), max_features=k),\n","        'Variance_Threshold': VarianceThreshold(threshold=0.1)\n","}\n","\n","\n","# =========================\n","# Pseudo-label classifiers + grids\n","# =========================\n","classifiers = {\n","    \"logreg\": (LogisticRegression(max_iter=500),\n","            {\"clf__C\": [0.01, 0.1, 1, 10]}),\n","    \"rf\": (RandomForestClassifier(),\n","           {\"clf__n_estimators\": [50, 100], \"clf__max_depth\": [None, 5, 10]}),\n","    # \"svc\": (SVC(probability=True),\n","    #         {\"clf__C\": [0.1, 1], \"clf__kernel\": [\"linear\", \"rbf\"]}),\n","    # \"nb\": (GaussianNB(), {}),\n","    # \"knn\": (KNeighborsClassifier(),\n","    #         {\"clf__n_neighbors\": [3, 5, 7]}),\n","    # \"dt\": (DecisionTreeClassifier(),\n","    #        {\"clf__max_depth\": [None, 5, 10]}),\n","    # \"gb\": (GradientBoostingClassifier(),\n","    #        {\"clf__n_estimators\": [50, 100]}),\n","    \"ab\": (AdaBoostClassifier(),\n","           {\"clf__n_estimators\": [50, 100]}),\n","    # \"xgb\": (XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\"),\n","    #         {\"clf__n_estimators\": [50, 100]}),\n","    # \"lgbm\": (LGBMClassifier(verbose=-1),\n","    #          {\"clf__n_estimators\": [50, 100]})\n","}\n","# =========================\n","# Final interpretable models\n","# =========================\n","final_models = {\n","\n","    \"logreg\": (LogisticRegression(max_iter=500),\n","            {\"clf__C\": [0.01, 0.1, 1, 10]}),\n","    \"rf\": (RandomForestClassifier(),\n","           {\"clf__n_estimators\": [50, 100], \"clf__max_depth\": [None, 5, 10]}),\n","    \"svc\": (SVC(probability=True),\n","            {\"clf__C\": [0.1, 1], \"clf__kernel\": [\"linear\", \"rbf\"]}),\n","    \"nb\": (GaussianNB(), {}),\n","    \"knn\": (KNeighborsClassifier(),\n","            {\"clf__n_neighbors\": [3, 5, 7]}),\n","    \"dt\": (DecisionTreeClassifier(),\n","           {\"clf__max_depth\": [None, 5, 10]}),\n","    \"gb\": (GradientBoostingClassifier(),\n","           {\"clf__n_estimators\": [50, 100]}),\n","    \"ab\": (AdaBoostClassifier(),\n","           {\"clf__n_estimators\": [50, 100]}),\n","    \"xgb\": (XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\"),\n","            {\"clf__n_estimators\": [50, 100]}),\n","    # \"lgbm\": (LGBMClassifier(),\n","    #          {\"clf__n_estimators\": [50, 100]})\n","}\n","\n","# =========================\n","# Metric functions\n","# =========================\n","def compute_metrics(model, X, y):\n","    preds = model.predict(X)\n","    proba = model.predict_proba(X)[:,1]\n","    return {\n","        \"accuracy\": accuracy_score(y, preds),\n","        \"f1\": f1_score(y, preds),\n","        \"roc_auc\": roc_auc_score(y, proba),\n","        \"precision\": precision_score(y, preds),\n","        \"recall\": recall_score(y, preds)\n","    }\n","\n","def crossval_metrics(model, X, y, cv=5):\n","    metrics = {}\n","    scorers = {\n","        \"accuracy\": \"accuracy\",\n","        \"f1\": \"f1\",\n","        \"roc_auc\": \"roc_auc\",\n","        \"precision\": \"precision\",\n","        \"recall\": \"recall\"\n","    }\n","    for m, sc in scorers.items():\n","        scores = cross_val_score(model, X, y, cv=cv, scoring=sc)\n","        metrics[f\"val_{m}_mean\"] = np.mean(scores)\n","        metrics[f\"val_{m}_std\"] = np.std(scores)\n","    return metrics\n","\n","\n","results = []\n","\n","save_dir = \"/content/drive/MyDrive/Projects/Ongoing Projects/Lung Outcome Prediciton_P20250048/Arman/Results/Feature Importance/\"\n","os.makedirs(save_dir, exist_ok=True)\n","\n","for sel_name, selector in feature_selectors.items():\n","    for clf_name, (clf, clf_grid) in classifiers.items():\n","        print(f\"Pseudo-labeling with FS={sel_name}, CLF={clf_name}\")\n","\n","        pseudo_pipe = Pipeline([\n","            ('pre', preprocessor),\n","            ('fs', selector),\n","            ('clf', clf)\n","        ])\n","\n","        if clf_grid:\n","            grid_pseudo = GridSearchCV(pseudo_pipe, clf_grid, cv=3, scoring=\"roc_auc\", n_jobs=-1)\n","            grid_pseudo.fit(X_labeled, y_labeled)\n","            best_pseudo = grid_pseudo.best_estimator_\n","        else:\n","            pseudo_pipe.fit(X_labeled, y_labeled)\n","            best_pseudo = pseudo_pipe\n","\n","        preds_unlabeled = best_pseudo.predict(X_unlabeled)\n","\n","        # Extended training data\n","        train_pseudo = pd.concat([\n","            train_labeled,\n","            train_unlabeled.assign(Outcome=preds_unlabeled)\n","        ])\n","        X_train_full = train_pseudo[feature_cols]\n","        y_train_full = train_pseudo[outcome_col].astype(int)\n","\n","        # Final models\n","        for final_name, (final_clf, final_grid) in final_models.items():\n","            print(\"Final Model is : \"+final_name)\n","            final_pipe = Pipeline([\n","                ('pre', preprocessor),\n","                ('fs', SelectKBest(f_classif, k=10)),  # fixed FS for final model\n","                ('clf', final_clf)\n","            ])\n","            grid_final = GridSearchCV(final_pipe, final_grid, cv=5, scoring='roc_auc', n_jobs=-1)\n","            grid_final.fit(X_train_full, y_train_full)\n","            best_final = grid_final.best_estimator_\n","\n","            # Test metrics\n","            test_metrics = compute_metrics(best_final, X_test, y_test)\n","            # Validation metrics\n","            val_metrics = crossval_metrics(best_final, X_train_full, y_train_full, cv=5)\n","\n","            res = {\n","                \"FS\": sel_name,\n","                \"PseudoCLF\": clf_name,\n","                \"FinalModel\": final_name,\n","                \"BestFinalParams\": str(grid_final.best_params_)\n","            }\n","            res.update({f\"test_{k}\": v for k, v in test_metrics.items()})\n","            res.update(val_metrics)\n","            results.append(res)\n","\n","            # =========================\n","            # SHAP feature importance\n","            # =========================\n","\n","            # extract fitted model and selected feature names\n","            fs_step = best_final.named_steps['fs']\n","            selected_idx = fs_step.get_support(indices=True)\n","            selected_features = [feature_cols[i] for i in selected_idx]\n","\n","            model_only = best_final.named_steps['clf']\n","\n","            X_sample = X_train_full.sample(20, random_state=42)  # to speed up\n","            X_proc = best_final.named_steps['pre'].transform(X_sample)\n","            X_proc_sel = fs_step.transform(X_proc)\n","\n","            # Assume best_final is your trained AdaBoostClassifier\n","\n","            explainer = shap.KernelExplainer(model_only.predict_proba, X_proc_sel)\n","            shap_values = explainer.shap_values(X_proc_sel)\n","\n","            # selected_features = feature_cols\n","            # model_only = best_final.named_steps['clf']\n","            # explainer = shap.Explainer(model_only, X_train_full)\n","            # shap_values = explainer(X_train_full)\n","\n","            # SHAP values for binary classification\n","            # shap_vals = shap_values.values\n","\n","            # Mean absolute SHAP toward each class\n","            feature_names = X_sample.columns\n","            class0_importance = np.abs(shap_values[0]).mean(axis=1)\n","            class1_importance = np.abs(shap_values[1]).mean(axis=1)\n","            print(selected_features)\n","            print(class0_importance)\n","            print(class1_importance)\n","\n","            df_imp = pd.DataFrame({\n","                \"Feature\": selected_features,\n","                \"Mean SHAP Class0\": class0_importance,\n","                \"Mean SHAP Class1\": class1_importance\n","            }).sort_values(\"Mean SHAP Class1\", ascending=False)\n","\n","            save_path = os.path.join(save_dir, f\"shap_importance_final={final_name}_fs={sel_name}_pl={clf_name}.csv\")\n","            df_imp.to_csv(save_path, index=False)\n","\n","            # =========================\n","            # Save results table\n","            # =========================\n","            results_df = pd.DataFrame(results)\n","            results_df.to_csv(\n","                os.path.join(save_dir, \"all_results_y2.csv\"), index=False\n","            )"],"metadata":{"id":"Z7htfnTwh6m8"},"execution_count":null,"outputs":[]}]}